Why might we ever consider bringing increasingly complex algorithms, with more storage and power overhead, to some of the smallest electronic devices, if their power is so low, and their storage capacity is already so limited, you might be rightfully wondering? Why not simply get more capable hardware and run everything remotely in a server room, where there are virtually no hardware constraints? There are several interesting reasons for approaching that process \textit{differently}.

\section{Motivation}
Firstly, embedded devices and systems have a relatively low production cost and have achieved a high-penetration rate in many markets and industries \cite{statista_embedded_market}. They are already widely-deployed, used and well-known. With this in mind, it is only logical to try and bring new capabilities to existing hardware, and extend its range of applications even further. \cite{tinyml_eetimes} \par
Secondly, we should look at the hardware of embedded devices: they are, in essence, micro-controllers (MCU's) with connected sensors, forming packages which are called, in Internet-of-Things (IOT) parlance, \textit{edge devices} \cite{tinyml_wheeler}. The implications can be significant: by bringing ML out of server rooms and closer to \textit{metal} level - we can achieve a high level of data filtration and quality earlier on in the data flow of our systems, while getting more efficient data processing and, very importantly in this day and age, increasing data privacy, since the data is already getting processed (maybe also anonymized) directly at sensor level, and parsed into a more meaningful format, instead of being dumped in close to raw format into a bigger, more vulnerable, network, to be first processed on an off-site, remote, system. \cite{edge_privacy} \par
These points should lay to rest most arguments against data processing right at the edge level of connected and IoT systems. Moreover, since data processing is, as we have seen in Chapter 2, a terrific application for Machine Learning, and considering the amount of interest in this topic at the present moment, we can safely conclude that tiny machine learning, \textit{TinyML}, is worth pursuing. On this note, let us dive into the technical aspects.

\section{Requirements of the Microcontroller, Edge Device World}
The \textit{TinyML} book defines its range of target devices as those being supplied with a power on the scale of \textit{milliwatts} or less, and, very importantly, running 32-bit architectures, having some few hundreds of kB's, or up to a maximum of some megabytes, of RAM and a capacity of binary flash storage on a similar scale. \cite{tinyml_def} \par
Given the above definition, we can directly formulate two main problems that have to be addressed if we want to have ML on an MCU: on the one hand, how do we overcome the performance limitations, and on the other hand, how do we fit these models into memory. \par

\subsection{What Happens in Memory}
In this section, we will offer an overlook of memory hierarchy of microcontrollers. With this, we hope that the role of the optimization steps undertaken later on will become more clear. \par
As we have seen in \textit{Chapter 2}, it is very important to optimize our applications in terms of memory use, to, for instance, capitalize on the principle of locality (also known as \textit{locality of reference}). \cite{memory_local} It is the tendency of processors to often access the same \textit{cache lines} repeatedly, which in turn makes it a smart move to keep the data in memory, and not delete it immediately after its first use, since those bits of data might actually be needed again after a short while, and it would be inefficient to have to load them again from RAM. \par
A typical edge device will often have a hierarchical memory structure, like most computer systems do. This memory will increase in terms of speed and decrease in terms of capacity, up from the high level of flash and RAM, into the CPU caches, and finally into the registers - which will be fastest and tiniest, where the actual computations actually take place. An overview of this memory hierarchy is offered in \textit{Table \ref{tab:memory_hierarchy}}. \par
There are, in essence, two types of memory locality: spatial and temporal. The first one refers to physically close or neighbouring cache segments tending to be accessed more often than those standing further apart, while the second one refers to keeping used data where it was originally loaded or even loading it into still faster memory segments, since this might be called upon again. \cite{computer_org_anddesign} \par
This is especially important for linear operations, such as the matrix multiplications we need for convolution, as our code will contain loops that reference arrays or tensors by indices (even if these indices might not always be increasing incrementally, as previously discussed). Let us take, for instance, matrices, the columns of which are represented in cache as sequences of rows, and we need, for our convolution, to recall one previously called column. Spatial locality will, in this case, be of great advantage. Or let us consider the case of needing the result of a previous addition again, for computing an average pooling layer. In this case, it is temporal locality that will help us.

\begin{table}[]
    \centering
        \begin{tabular}{|c|c|c|} \hline
            Memory Type & Average Capacity & Access Times \\\hline
            CPU Registers & 8 - 256 registers & instant \\\hline
            L1 Cache & 32 kB & very fast \\\hline
            L2 Cache & ~ 128 kB & fast \\\hline
            L3 Cache & ~256 kB & slower \\\hline
            RAM & ~512 kB & slower still \\\hline
            Flash & few MB & slowest \\\hline
        \end{tabular}
    \caption{Memory Hierarchy of an edge device. Source: \cite{stm32_cache}}
    \label{tab:memory_hierarchy}
\end{table}

\subsection{Compressing Full-Size Models}
In this section, we will address three specific post-training solutions to the above named size-related problems of memory and performance: \textit{Quantization}, \textit{Pruning} and \textit{Compression}. \par
\begin{itemize}
    \item \textbf{Quantization}: TensorFlow Lite offers 3 different types of post-training quantization: dynamic range, full integer and float-16; we will primarily focus on full integer quantization, since it is most appropriate for Edge devices, while the other two are more targeted at smartphone applications, and therefore focus less on simple size reduction. Full integer quantization offer a reduction of up to 4 times in size, with, at the same time, up to 3 times faster inference. Both improvements stem primarily from the fact that the model weights are translated from 32-bit floating point numbers down to 8-bit integers, which literally means that the model is using up around 4 times less bits in memory.  \cite{tflite_quantization} There will, obviously, be a certain degree of reduction in the dynamic range of the model, but this is a compromise we can account and prepare for. There are also mixed quantization modes, such as 16-bit activations with 8-bit weights.
    \item \textbf{Pruning}: TensorFlow Lite also offers, in addition to quantization, the option to \textit{remove} parameters from a model that only had a small impact on its output. This way, we can reduce size even further. It should be noted that pruning must be done before quantization. Pruning essentially eliminates weights that are close to zero, which are especially useful for audio applications, such as speech recognition, where we only really need to focus on those areas of the sound spectrum where human voices are located in terms of frequency. Pruning or trimming insignificant weights from a model results in a \textit{sparse} model, which is both smaller in size and faster to run. \cite{tflite_pruning}
    \item \textbf{Compression}: This refers to a new representation of the model in memory as a \textit{flat buffer} instead of an oriented graph, as we would have represented it as a neural network. In other words, a TensorFlow Lite model architecture, which was originally a higher-dimensional graph with a complicated edge and node structure, will now be represented as a linear array, which an interpreter can recompose back into the original graph representation. \cite{google_prot_buf}
\end{itemize}

We have now become familiar with the tools that we need for running \textit{TinyML} and how they overcome the limitations of the embedded world. Therefore, in the next chapter, we will look into the problem of face detection and how it can be addressed with ML techniques. Afterwards, in Chapter 5, we will dive into \textit{TinyFace} and see how we can combine everything we have touched on so far.
