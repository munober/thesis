% \chapter{Discussion} % maybe the part with automatic optimization
In the previous chapters, we have looked in broader terms at the problems we are faced with when trying to make \textit{TinyFace} a reality. In this chapter, we will look into the combination of decisions and parameter choices that have enabled us to accomplish our goal.

\section{The Final Dataset}
The model has then been trained on a custom generated dataset, containing data that has been scraped from the web, on a total of 30,000 images - 14,000 faces and 16,000 non-face objects - as such we have a binary classifier. Each image is \textit{100 * 100} pixels large and grayscale. Moreover, each image has been contrast-normalized to a range of 0 to 10, to minimize contrast variance, which would have otherwise been seriously impeding the model's performance during real-world use if left as it had been by default in the images (0-255). A side effect of this, it should be noted, is that the resulting training images look \textit{pitch black} to the naked eye, and we can barely make out any details if at all since there is so little detail left for the human eye to pick up on. However, it is - in this case - advantageous to have so little dynamic range, since the neural network can use its resources to learn the actual geometric shapes and patterns of the faces, instead of wasting valuable memory on learning less-relevant contrast differences (for instance, a soft shadow to the side of one's nose) or, in other words, learning \textit{shades of gray} unnecessarily. \par

\section{Optimized SqueezeNet}
When choosing which model architecture to use, the author initially experimented with color images. However, these only really made the model size increase so drastically that the finished files could only barely fit inside the memory of the embedded device. As such, the decision was made to switch to exclusively gray-scale images. This also meant the model complexity was reduced since the convolutional layers only had to be applied to one dimension, instead of three. \par
The final model was, as previously mentioned, based on SqueezeNet. The final trained model file is \textit{139 KB} large, after having been compressed and quantified to 8-bit weights. \par
\section{The Golden Training Formula}
After repeatedly experimenting with different numbers and combinations for each of the hyperparameters, the author has settled on a set of values that delivered the highest accuracy. The hyperparameters for the most successful training sessions have been set as follows: 
\begin{itemize}
    \item 5 training epochs
    \item a batch size of 32 samples
    \item a 75\%-25\% divide between training and test data
    \item a dropout rate of 10\%
\end{itemize}
The choice of hyperparameters and the evaluation of training performance is a lengthy process, and a relatively big time drain, in general, plus highly repetitive. That makes it a prime candidate for automation. A solution for this will be proposed in the next chapter.
\section{Discussing the Results}
Let us, therefore, close this chapter by looking back at what has been achieved. We have so far presented a complete, end-to-end, process for running face detection in a TensorFlow Lite interpreter. We started out with compiling our own dataset and developing our own model architecture, then optimized size and performance, deployed to the interpreter, and evaluated performance. As such, we have created a new application of \textit{TinyML}, that we have called \textit{TinyFace}. With this in mind, and having now accomplished what we set out to do, let us close this chapter and proceed to the final one, where we will present possible future research directions, and deliver our closing remarks.